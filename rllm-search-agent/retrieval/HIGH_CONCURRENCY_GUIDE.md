# é«˜å¹¶å‘æ£€ç´¢æœåŠ¡ä¼˜åŒ–æŒ‡å—

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

æ‚¨é‡åˆ°çš„å†…å­˜æº¢å‡ºé—®é¢˜ï¼ˆOOMï¼‰æ˜¯ç”±äºä½¿ç”¨äº† **10 workers**ï¼Œæ¯ä¸ª worker ç‹¬ç«‹åŠ è½½æ¨¡å‹å’Œç´¢å¼•ï¼š

```
10 workers Ã— 85GB/worker = 850GB å†…å­˜å ç”¨ âŒ
```

è¿™è¶…è¿‡äº†æ‚¨èŠ‚ç‚¹çš„å¯ç”¨å†…å­˜ï¼ˆ896GBï¼‰ï¼Œå¯¼è‡´ Ray æ€æ­»è¿›ç¨‹ã€‚

## âœ… æ¨èè§£å†³æ–¹æ¡ˆï¼šå• Worker + å¼‚æ­¥å¹¶å‘

### æ¶æ„å¯¹æ¯”

| æ–¹æ¡ˆ | å†…å­˜å ç”¨ | å¹¶å‘èƒ½åŠ› | ååé‡ | æ¨è |
|------|---------|---------|--------|------|
| **10 workers (æ—§)** | ~850GB | é«˜ | é«˜ | âŒ å†…å­˜æº¢å‡º |
| **1 worker + async (æ–°)** | ~85GB | é«˜ | é«˜ | âœ… æ¨è |
| æ¨¡å‹æœåŠ¡åˆ†ç¦» | ~85GB | æé«˜ | æé«˜ | ğŸ”„ å¤æ‚åœºæ™¯ |

### ä¸ºä»€ä¹ˆå• Worker ä¹Ÿèƒ½é«˜å¹¶å‘ï¼Ÿ

ç°åœ¨çš„æœåŠ¡å™¨å·²ç»ä¼˜åŒ–ä¸º**å¼‚æ­¥æ¶æ„**ï¼š

```python
# å…³é”®ä¼˜åŒ–ç‚¹
1. async/await å¼‚æ­¥å¤„ç†
2. CPU å¯†é›†æ“ä½œåœ¨çº¿ç¨‹æ± æ‰§è¡Œï¼ˆä¸é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
3. FAISS æœç´¢å¼‚æ­¥åŒ–
4. æ¨¡å‹ç¼–ç å¼‚æ­¥åŒ–
```

### æ€§èƒ½é¢„æœŸ

ä½¿ç”¨ **1 worker + async æ¨¡å¼**ï¼š

- **å†…å­˜å ç”¨**: ~85GBï¼ˆèŠ‚çœ 90%ï¼‰
- **å¹¶å‘å¤„ç†**: 100+ å¹¶å‘è¯·æ±‚
- **ååé‡**: 10-50 req/sï¼ˆå–å†³äºæŸ¥è¯¢å¤æ‚åº¦ï¼‰
- **å»¶è¿Ÿ**: 100-500ms/è¯·æ±‚

## ğŸš€ å¿«é€Ÿå¯åŠ¨

### æ–¹æ¡ˆ 1: ä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# å• worker æ¨¡å¼ï¼ˆé»˜è®¤ï¼Œæ¨èï¼‰
bash retrieval/launch_server.sh

# æŒ‡å®šç«¯å£
bash retrieval/launch_server.sh /path/to/data 8080

# å¼€å‘æ¨¡å¼ï¼ˆè‡ªåŠ¨é‡è½½ï¼‰
bash retrieval/launch_server.sh /path/to/data 8080 1 reload
```

### æ–¹æ¡ˆ 2: ç›´æ¥è¿è¡Œ Python

```bash
# å• workerï¼ˆæ¨èï¼‰
python retrieval/server.py --workers 1 --host 0.0.0.0 --port 2727

# å¦‚æœçœŸçš„éœ€è¦å¤š workerï¼ˆéœ€ç¡®ä¿æœ‰è¶³å¤Ÿå†…å­˜ï¼‰
python retrieval/server.py --workers 2 --host 0.0.0.0 --port 2727
# éœ€è¦: 2 Ã— 85GB = 170GB å¯ç”¨å†…å­˜
```

## ğŸ“Š æ€§èƒ½æµ‹è¯•

### è¿è¡Œå¹¶å‘æµ‹è¯•

```bash
# å®‰è£…ä¾èµ–
pip install aiohttp

# æµ‹è¯• 100 ä¸ªè¯·æ±‚ï¼Œ50 å¹¶å‘
python retrieval/benchmark_concurrency.py --server http://localhost:2727 --requests 100 --concurrent 50

# æµ‹è¯• 1000 ä¸ªè¯·æ±‚ï¼Œ100 å¹¶å‘
python retrieval/benchmark_concurrency.py --requests 1000 --concurrent 100
```

### é¢„æœŸç»“æœï¼ˆå• workerï¼‰

```
æ€»è¯·æ±‚: 1000
å¹¶å‘æ•°: 100
æ€»è€—æ—¶: ~50-100ç§’
ååé‡: 10-20 req/s
æˆåŠŸç‡: >99%
å¹³å‡å»¶è¿Ÿ: 200-500ms
```

## ğŸ”§ é«˜çº§ä¼˜åŒ–é€‰é¡¹

### 1. è°ƒæ•´ uvicorn å‚æ•°

```bash
# å¢åŠ æ¯ä¸ª worker çš„è¿æ¥æ•°é™åˆ¶
python retrieval/server.py \
    --workers 1 \
    --host 0.0.0.0 \
    --port 2727 \
    --limit-concurrency 1000 \
    --limit-max-requests 10000
```

### 2. ç³»ç»Ÿçº§ä¼˜åŒ–

```bash
# å¢åŠ æ–‡ä»¶æè¿°ç¬¦é™åˆ¶
ulimit -n 65536

# è°ƒæ•´ TCP å‚æ•°ï¼ˆ/etc/sysctl.confï¼‰
net.core.somaxconn = 4096
net.ipv4.tcp_max_syn_backlog = 8192
```

### 3. FAISS ä¼˜åŒ–

å¦‚æœéœ€è¦æ›´é«˜æ€§èƒ½ï¼Œè€ƒè™‘ï¼š

```python
# ä½¿ç”¨ GPU FAISSï¼ˆå¦‚æœæœ‰ GPUï¼‰
import faiss
gpu_res = faiss.StandardGpuResources()
index = faiss.index_cpu_to_gpu(gpu_res, 0, cpu_index)

# ä½¿ç”¨æ›´å¿«çš„ç´¢å¼•ç±»å‹
# IVF ç´¢å¼•ï¼šå¿«é€Ÿä½†ç•¥å¾®æŸå¤±ç²¾åº¦
# HNSW ç´¢å¼•ï¼šå¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦
```

## ğŸ—ï¸ æ–¹æ¡ˆ 3: æ¨¡å‹æœåŠ¡åˆ†ç¦»ï¼ˆè¶…é«˜å¹¶å‘åœºæ™¯ï¼‰

å¦‚æœå• worker ä»ä¸æ»¡è¶³éœ€æ±‚ï¼Œå¯ä»¥å°†æ¨¡å‹æœåŠ¡ç‹¬ç«‹éƒ¨ç½²ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Gateway    â”‚  (å¤šå®ä¾‹ï¼Œè½»é‡çº§)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â†“         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model  â”‚ â”‚ Model  â”‚  (é‡é‡çº§æœåŠ¡ï¼Œ1-2å®ä¾‹)
â”‚Service â”‚ â”‚Service â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å®ç°æ–¹æ¡ˆ

```python
# 1. ç‹¬ç«‹çš„æ¨¡å‹æœåŠ¡ï¼ˆä½¿ç”¨ Ray Serve æˆ– Tritonï¼‰
# 2. API ç½‘å…³è°ƒç”¨æ¨¡å‹æœåŠ¡
# 3. è´Ÿè½½å‡è¡¡

# ä¼˜ç‚¹ï¼š
# - æ¨¡å‹å®ä¾‹æ•°é‡å¯ç‹¬ç«‹æ§åˆ¶
# - æ›´å¥½çš„èµ„æºåˆ©ç”¨
# - æ”¯æŒæ›´å¤æ‚çš„éƒ¨ç½²ç­–ç•¥

# ç¼ºç‚¹ï¼š
# - æ¶æ„æ›´å¤æ‚
# - ç½‘ç»œå»¶è¿Ÿå¢åŠ 
```

## ğŸ“ˆ ç›‘æ§å’Œè°ƒä¼˜

### ç›‘æ§å…³é”®æŒ‡æ ‡

```bash
# 1. å†…å­˜ä½¿ç”¨
watch -n 1 'ps aux | grep server.py | grep -v grep'

# 2. è¿æ¥æ•°
netstat -an | grep :2727 | wc -l

# 3. ç³»ç»Ÿè´Ÿè½½
htop
```

### ä½¿ç”¨ FastAPI å†…ç½®ç›‘æ§

è®¿é—®ä»¥ä¸‹ç«¯ç‚¹ï¼š
- `http://localhost:2727/docs` - API æ–‡æ¡£
- `http://localhost:2727/health` - å¥åº·æ£€æŸ¥
- `http://localhost:2727/openapi.json` - OpenAPI è§„èŒƒ

### æ·»åŠ  Prometheus ç›‘æ§ï¼ˆå¯é€‰ï¼‰

```python
# å®‰è£…
pip install prometheus-fastapi-instrumentator

# åœ¨ server.py æ·»åŠ 
from prometheus_fastapi_instrumentator import Instrumentator

Instrumentator().instrument(app).expose(app)
```

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

### âœ… æ¨èé…ç½®

```bash
# ç”Ÿäº§ç¯å¢ƒ
python retrieval/server.py \
    --workers 1 \
    --host 0.0.0.0 \
    --port 2727 \
    --log-level info

# å†…å­˜å ç”¨: ~85GB
# å¯å¤„ç†: 50-100 å¹¶å‘è¯·æ±‚
# é€‚ç”¨äº: å¤§å¤šæ•°ç”Ÿäº§åœºæ™¯
```

### âŒ é¿å…çš„é…ç½®

```bash
# ä¸æ¨èï¼šå¤š worker æ¨¡å¼ï¼ˆé™¤éæœ‰å……è¶³å†…å­˜ï¼‰
python retrieval/server.py --workers 10  # éœ€è¦ 850GB å†…å­˜ï¼

# ä¸æ¨èï¼šåŒæ­¥æ¨¡å¼ï¼ˆå·²åºŸå¼ƒï¼‰
# æ—§ç‰ˆæœ¬ä½¿ç”¨åŒæ­¥æœç´¢ï¼Œæ€§èƒ½å·®
```

### ğŸ”„ ä½•æ—¶éœ€è¦å¤š Worker

ä»…åœ¨ä»¥ä¸‹æƒ…å†µè€ƒè™‘å¤š workerï¼š

1. **å……è¶³å†…å­˜**: ç¡®ä¿æœ‰ `workers Ã— 85GB` å¯ç”¨å†…å­˜
2. **CPU å¯†é›†**: ä»»åŠ¡ä¸»è¦æ˜¯ CPU è®¡ç®—ï¼ˆä¸æ˜¯ I/Oï¼‰
3. **å·²éªŒè¯ç“¶é¢ˆ**: é€šè¿‡æµ‹è¯•ç¡®è®¤å• worker æ˜¯ç“¶é¢ˆ
4. **ç›‘æ§åˆ°ä½**: æœ‰å®Œå–„çš„å†…å­˜ç›‘æ§å’Œå‘Šè­¦

```bash
# 2 workers é…ç½®ï¼ˆéœ€è¦ 170GB å¯ç”¨å†…å­˜ï¼‰
python retrieval/server.py --workers 2 --host 0.0.0.0

# ä½¿ç”¨å‰å…ˆæµ‹è¯•å†…å­˜
free -h
# ç¡®ä¿ available memory > 170GB
```

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: ä»ç„¶ OOM

**æ£€æŸ¥**:
```bash
# æŸ¥çœ‹å®é™… worker æ•°é‡
ps aux | grep "server.py" | grep -v grep

# æŸ¥çœ‹å†…å­˜å ç”¨
ps aux | grep python | awk '{sum+=$6} END {print sum/1024/1024 " GB"}'
```

**è§£å†³**:
- ç¡®è®¤åªå¯åŠ¨äº† 1 ä¸ª server å®ä¾‹
- æ£€æŸ¥å…¶ä»– Python è¿›ç¨‹æ˜¯å¦å ç”¨å†…å­˜
- è€ƒè™‘é‡å¯æœåŠ¡å™¨æ¸…ç†å†…å­˜

### é—®é¢˜ 2: æ€§èƒ½ä¸ä½³

**æ£€æŸ¥**:
```bash
# è¿è¡Œæ€§èƒ½æµ‹è¯•
python retrieval/benchmark_concurrency.py --concurrent 50

# æ£€æŸ¥ CPU ä½¿ç”¨ç‡
htop
```

**ä¼˜åŒ–**:
- å¦‚æœ CPU æ»¡è½½ï¼šè€ƒè™‘å¢åŠ  workerï¼ˆéœ€è¦æ›´å¤šå†…å­˜ï¼‰
- å¦‚æœ CPU ä¸æ»¡ï¼šæ£€æŸ¥ç½‘ç»œ/ç£ç›˜ç“¶é¢ˆ
- ä¼˜åŒ–æŸ¥è¯¢ï¼šå‡å°‘ top_k å‚æ•°

### é—®é¢˜ 3: è¿æ¥è¢«æ‹’ç»

**æ£€æŸ¥**:
```bash
# æŸ¥çœ‹æœ€å¤§è¿æ¥æ•°
ulimit -n

# æŸ¥çœ‹å½“å‰è¿æ¥
netstat -an | grep :2727 | grep ESTABLISHED | wc -l
```

**è§£å†³**:
```bash
# å¢åŠ æ–‡ä»¶æè¿°ç¬¦é™åˆ¶
ulimit -n 65536

# æˆ–åœ¨ /etc/security/limits.conf æ°¸ä¹…è®¾ç½®
* soft nofile 65536
* hard nofile 65536
```

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. æŸ¥çœ‹æ—¥å¿—: æœåŠ¡å™¨ä¼šè¾“å‡ºè¯¦ç»†çš„å¯åŠ¨å’Œè¿è¡Œæ—¥å¿—
2. è¿è¡Œæµ‹è¯•: ä½¿ç”¨ `benchmark_concurrency.py` éªŒè¯æ€§èƒ½
3. æ£€æŸ¥å¥åº·: è®¿é—® `/health` ç«¯ç‚¹ç¡®è®¤æœåŠ¡çŠ¶æ€

## ğŸ“ æŠ€æœ¯ç»†èŠ‚

### å¼‚æ­¥æ¶æ„åŸç†

```python
# ä¼ ç»ŸåŒæ­¥æ¨¡å¼ï¼ˆé˜»å¡ï¼‰
def search(query):
    vector = model.encode(query)      # é˜»å¡ 100ms
    results = index.search(vector)     # é˜»å¡ 50ms
    return results
# æ€»å»¶è¿Ÿ: 150msï¼Œå…¶é—´æ— æ³•å¤„ç†å…¶ä»–è¯·æ±‚

# æ–°å¼‚æ­¥æ¨¡å¼ï¼ˆéé˜»å¡ï¼‰
async def search_async(query):
    # åœ¨çº¿ç¨‹æ± æ‰§è¡Œï¼Œä¸é˜»å¡äº‹ä»¶å¾ªç¯
    vector = await run_in_executor(model.encode, query)
    results = await run_in_executor(index.search, vector)
    return results
# æ€»å»¶è¿Ÿ: 150msï¼Œä½†æœŸé—´å¯ä»¥å¤„ç†å…¶ä»–è¯·æ±‚ï¼
```

### å¹¶å‘å¤„ç†ç¤ºä¾‹

```
æ—¶é—´è½´:
0ms:    è¯·æ±‚1å¼€å§‹ â†’ æäº¤åˆ°çº¿ç¨‹æ± 
10ms:   è¯·æ±‚2å¼€å§‹ â†’ æäº¤åˆ°çº¿ç¨‹æ± 
20ms:   è¯·æ±‚3å¼€å§‹ â†’ æäº¤åˆ°çº¿ç¨‹æ± 
...
150ms:  è¯·æ±‚1å®Œæˆ
160ms:  è¯·æ±‚2å®Œæˆ
170ms:  è¯·æ±‚3å®Œæˆ

å• worker åœ¨ 170ms å†…å¤„ç†äº† 3 ä¸ªè¯·æ±‚ï¼
å¦‚æœæ˜¯åŒæ­¥æ¨¡å¼ï¼Œéœ€è¦ 450msï¼ˆ150ms Ã— 3ï¼‰
```

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå• worker ä¹Ÿèƒ½é«˜å¹¶å‘çš„åŸå› ã€‚

